{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based Collaborative Filtering\n",
    "For a given user $ u $ and item $ i $, predict the rating $ r_{ui} $ as follows:\n",
    "\n",
    "$$\n",
    "r_{ui} = \\frac{\\sum_{v \\in N_u} \\text{sim}(u, v) \\cdot r_{vi}}{\\sum_{v \\in N_u} |\\text{sim}(u, v)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N_u $: Users who rated item $ i $.\n",
    "- $ \\text{sim}(u, v) $: Similarity between users $ u $ and $ v $.\n",
    "- $ r_{vi} $: Rating of user $ v $ for item $ i $.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the idea\n",
    "# collaborative filtering:\n",
    "\n",
    "# given a user x and an item i, estimate rating r(i) by:\n",
    "# 1. finding a set of users U who rated the same items as x \n",
    "# 2. aggregate the ratings of i provided by Nu\n",
    "\n",
    "# In practice:\n",
    "# One considers all items user x has already rated, then other users are searched who have rated these items,\n",
    "# but also have rated the new item for which we do not know the rating yet of user x. In this way users with similar\n",
    "# interests are selected.\n",
    "\n",
    "# Similarity between users can be calculated by cosine, or pearson similarity. \n",
    "# one downside for pearson - it is not defined when the variance of the user ratings is 0. e.g. all ratings are 2.5.\n",
    "\n",
    "# The aggregation function: whether the neighbors rating for the unseen item i, are higher or lower than their average.\n",
    "# .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7260</td>\n",
       "      <td>20145</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243238</td>\n",
       "      <td>85182</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9135</td>\n",
       "      <td>45973</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18671</td>\n",
       "      <td>63554</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>243293</td>\n",
       "      <td>81002</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  user_id  rating\n",
       "0     7260    20145     3.5\n",
       "1   243238    85182     4.0\n",
       "2     9135    45973     1.0\n",
       "3    18671    63554     3.0\n",
       "4   243293    81002     5.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id\n",
       "408       257\n",
       "748       213\n",
       "522       149\n",
       "356       142\n",
       "26        142\n",
       "         ... \n",
       "247693      1\n",
       "248107      1\n",
       "245643      1\n",
       "246570      1\n",
       "246356      1\n",
       "Name: count, Length: 15712, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['book_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "3785     2041\n",
       "28251     524\n",
       "43652     350\n",
       "5180      345\n",
       "27445     266\n",
       "         ... \n",
       "87162       1\n",
       "83607       1\n",
       "79107       1\n",
       "89349       1\n",
       "87278       1\n",
       "Name: count, Length: 18905, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = train_df.pivot(index='user_id', columns='book_id', values='rating')\n",
    "user_item_matrix.fillna(0, inplace=True)\n",
    "user_item_sparse = csr_matrix(user_item_matrix) # to sparse\n",
    "user_similarity = cosine_similarity(user_item_sparse) # cos sim between users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, book_id, user_item_matrix, user_similarity):\n",
    "    user_idx = user_item_matrix.index.get_loc(user_id)\n",
    "    item_idx = user_item_matrix.columns.get_loc(book_id)\n",
    "    item_ratings = user_item_matrix.iloc[:, item_idx]\n",
    "    neighbors = item_ratings[item_ratings > 0].index\n",
    "    neighbor_idxs = [user_item_matrix.index.get_loc(neighbor) for neighbor in neighbors]\n",
    "    similarities = user_similarity[user_idx, neighbor_idxs]\n",
    "    ratings = item_ratings[neighbors]\n",
    "    numerator = np.dot(similarities, ratings)\n",
    "    denominator = np.sum(np.abs(similarities))\n",
    "\n",
    "    if denominator == 0:\n",
    "        return np.nan  \n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def predict_test_ratings(test_df, user_item_matrix, user_similarity, batch_size=128):\n",
    "    user_item_np = user_item_matrix.to_numpy()\n",
    "    mean_rating = user_item_np[user_item_np > 0].mean()\n",
    "    \n",
    "    user_to_idx = {user: idx for idx, user in enumerate(user_item_matrix.index)}\n",
    "    book_to_idx = {book: idx for idx, book in enumerate(user_item_matrix.columns)}\n",
    "    \n",
    "    predictions = []\n",
    "    num_batches = len(test_df) // batch_size + int(len(test_df) % batch_size > 0)\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Predicting test set in batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(test_df))\n",
    "        \n",
    "        batch = test_df.iloc[start_idx:end_idx]\n",
    "        user_indices = batch['user_id'].map(user_to_idx).to_numpy()\n",
    "        book_indices = batch['book_id'].map(book_to_idx).to_numpy()\n",
    "        valid_mask = (~np.isnan(user_indices)) & (~np.isnan(book_indices))\n",
    "        batch_predictions = np.full(len(batch), mean_rating, dtype=np.float32)\n",
    "        \n",
    "        valid_user_indices = user_indices[valid_mask].astype(int)\n",
    "        valid_book_indices = book_indices[valid_mask].astype(int)\n",
    "\n",
    "        user_similarities = user_similarity[valid_user_indices, :]  \n",
    "        item_ratings = user_item_np[:, valid_book_indices] \n",
    "        item_ratings_mask = item_ratings > 0\n",
    "        weighted_ratings = np.dot(user_similarities, item_ratings * item_ratings_mask)\n",
    "        similarity_sums = np.dot(user_similarities, item_ratings_mask)\n",
    "\n",
    "        predictions_valid = np.divide(\n",
    "            weighted_ratings,\n",
    "            similarity_sums,\n",
    "            out=np.full_like(weighted_ratings, mean_rating),\n",
    "            where=similarity_sums > 0\n",
    "        )\n",
    "\n",
    "        batch_predictions[valid_mask] = predictions_valid.diagonal()\n",
    "        predictions.extend(zip(batch['id'], batch_predictions))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting test set in batches: 100%|██████████| 230/230 [00:16<00:00, 14.01it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_test_ratings(test_df, user_item_matrix, user_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29366, 2.3218882)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame(predictions, columns=['id', 'rating'])\n",
    "output_csv_path = 'predictions.csv'\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Predictions saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
