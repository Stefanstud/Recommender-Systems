{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# [I 2024-12-13 01:18:39,043] Trial 39 finished with value: 0.8016925942757998 and parameters: {'EMBED_DIM': 64, 'LR': 3.591388314145999e-05, 'REG_LAMBDA': 0.001630666920706239, 'BATCH_SIZE': 64, 'PATIENCE': 14}. Best is trial 39 with value: 0.8016925942757998.\n",
    "\n",
    "# -----------------------\n",
    "# Hyperparameters\n",
    "# -----------------------\n",
    "EMBED_DIM = 64\n",
    "BATCH_SIZE = 64\n",
    "LR = 3.591388314145999e-05\n",
    "EPOCHS = 100\n",
    "REG_LAMBDA = 1e-1\n",
    "PATIENCE = 14  # Early stopping patience\n",
    "\n",
    "# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# -----------------------\n",
    "# Data Loading\n",
    "# -----------------------\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\") \n",
    "\n",
    "unique_users = train_df.user_id.unique()\n",
    "unique_books = train_df.book_id.unique()\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "book2idx = {b: i for i, b in enumerate(unique_books)}\n",
    "\n",
    "train_df[\"user_idx\"] = train_df[\"user_id\"].map(user2idx)\n",
    "train_df[\"book_idx\"] = train_df[\"book_id\"].map(book2idx)\n",
    "\n",
    "n_users = len(user2idx)\n",
    "n_books = len(book2idx)\n",
    "\n",
    "train_df['user_idx'] = train_df['user_id'].map(user2idx)\n",
    "train_df['book_idx'] = train_df['book_id'].map(book2idx)\n",
    "\n",
    "\n",
    "# drop low interaction users from validation set\n",
    "# low interaction after combining train and test \n",
    "data = pd.concat([train_df, test_df])\n",
    "low_interaction_users = data['user_idx'].value_counts()[data['user_idx'].value_counts() < 5].index\n",
    "\n",
    "# Create validation split\n",
    "user_counts = train_df['user_id'].value_counts()\n",
    "rare_threshold = 4\n",
    "rare_users = user_counts[user_counts <= rare_threshold].index\n",
    "non_rare_users = user_counts[user_counts > rare_threshold].index\n",
    "rare_df = train_df[train_df['user_id'].isin(rare_users)]\n",
    "non_rare_df = train_df[train_df['user_id'].isin(non_rare_users)]\n",
    "train_non_rare, val_non_rare = train_test_split(\n",
    "    non_rare_df, test_size=0.1, stratify=non_rare_df['user_id'], random_state=42\n",
    ")\n",
    "rare_train, rare_val = train_test_split(\n",
    "    rare_df, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_data = pd.concat([train_non_rare, rare_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_data = pd.concat([val_non_rare, rare_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = df[\"user_idx\"].values\n",
    "        self.books = df[\"book_idx\"].values\n",
    "        self.ratings = df[\"rating\"].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.users[idx], dtype=torch.long),\n",
    "            torch.tensor(self.books[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ratings[idx], dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Fixed Offset based on User Counts\n",
    "# -----------------------\n",
    "user_counts = (\n",
    "    train_data.groupby(\"user_idx\").size().reindex(range(n_users), fill_value=0).values\n",
    ")\n",
    "max_count = user_counts.max()\n",
    "# normalize count: f_u in [0,1]\n",
    "normalized_counts = np.log1p(user_counts) / np.log1p(max_count)\n",
    "\n",
    "# Define fixed offset: from ~3.66 at f_u=0 to ~2.10 at f_u=1\n",
    "offset_high = 3.66\n",
    "offset_low = 2.10\n",
    "# offset(u) = offset_high - (offset_high - offset_low)*f_u\n",
    "# = offset_high + (offset_low - offset_high)*f_u\n",
    "offsets = offset_high + (offset_low - offset_high) * normalized_counts\n",
    "offsets_tensor = torch.tensor(offsets, dtype=torch.float32, device=DEVICE)\n",
    "# TODO I will get the offsets directly based on the avg rating of the users with the same number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_counts = train_data.groupby(\"user_id\")[\"user_id\"].transform(\"size\")\n",
    "count2mean = train_data.groupby(user_id_counts)[\"rating\"].mean()\n",
    "count2mean_dict = count2mean.to_dict()\n",
    "# count2mean_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define old offset high/low\n",
    "offset_high = 3.66\n",
    "offset_low = 2.10\n",
    "\n",
    "# Compute the mean rating by exact user count\n",
    "user_id_counts = train_data.groupby(\"user_id\")[\"user_id\"].transform(\"size\")\n",
    "count2mean = train_data.groupby(user_id_counts)[\"rating\"].mean()\n",
    "count2mean_dict = count2mean.to_dict()\n",
    "\n",
    "# Compute offsets now with lookup and fallback\n",
    "offsets = np.zeros_like(user_counts, dtype=np.float32)\n",
    "for i, c in enumerate(user_counts):\n",
    "    if c in count2mean_dict:\n",
    "        # Use the mean for that count minus global mean as offset\n",
    "        offsets[i] = count2mean_dict[c]\n",
    "    else:\n",
    "        # Fallback to old linear approximation based on normalized counts\n",
    "        f_u = np.log1p(c) / (np.log1p(max_count) if max_count > 0 else 1.0)\n",
    "        offsets[i] = offset_high + (offset_low - offset_high) * f_u\n",
    "\n",
    "offsets_tensor = torch.tensor(offsets, dtype=torch.float32, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Model Definition\n",
    "# -----------------------\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=64):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        self.mlp1 = nn.Linear(emb_dim, emb_dim)\n",
    "        self.mlp2 = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, user_offsets):\n",
    "        U = self.user_emb(user_ids)\n",
    "        V = self.item_emb(item_ids)\n",
    "        U = self.mlp1(U) + U # add skip connection\n",
    "        V = self.mlp2(V) + V # add skip connection\n",
    "        u_b = self.user_bias(user_ids).squeeze()\n",
    "        i_b = self.item_bias(item_ids).squeeze()\n",
    "        \n",
    "        # Add the fixed offset from user_counts\n",
    "        offset = user_offsets[user_ids]\n",
    "        pred = (U * V).sum(dim=1) + u_b + i_b + self.global_bias + offset\n",
    "        # Relu \n",
    "        pred = nn.functional.relu(pred)\n",
    "        return pred\n",
    "\n",
    "def loss_fn(pred, target, model, reg_lambda, loss_type=\"L1\"):\n",
    "    if loss_type == \"L1\":\n",
    "        base_loss = nn.L1Loss()(pred, target)\n",
    "    elif loss_type == \"MSE\":\n",
    "        base_loss = nn.MSELoss()(pred, target)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid loss type. Choose 'L1' or 'MSE'.\")\n",
    "    \n",
    "    # L2 regularization on embeddings\n",
    "    user_reg = model.user_emb.weight.norm(2)\n",
    "    item_reg = model.item_emb.weight.norm(2)\n",
    "    return base_loss + reg_lambda * (user_reg + item_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, scheduler, epochs, patience, save_path):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_squared_error = 0\n",
    "        num_train_samples = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for users, items, ratings in train_loader:\n",
    "            users = users.to(DEVICE)\n",
    "            items = items.to(DEVICE)\n",
    "            ratings = ratings.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(users, items, offsets_tensor)\n",
    "            loss = loss_fn(preds, ratings, model, REG_LAMBDA)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Accumulate squared error for RMSE\n",
    "            train_squared_error += ((preds - ratings) ** 2).sum().item()\n",
    "            num_train_samples += ratings.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_rmse = np.sqrt(train_squared_error / num_train_samples)\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_squared_error = 0\n",
    "        num_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for users, items, ratings in val_loader:\n",
    "                users = users.to(DEVICE)\n",
    "                items = items.to(DEVICE)\n",
    "                ratings = ratings.to(DEVICE)\n",
    "\n",
    "                preds = model(users, items, offsets_tensor)\n",
    "                val_loss = loss_fn(preds, ratings, model, 0)  # No regularization\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Accumulate squared error for RMSE\n",
    "                val_squared_error += ((preds - ratings) ** 2).sum().item()\n",
    "                num_val_samples += ratings.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_rmse = np.sqrt(val_squared_error / num_val_samples)\n",
    "\n",
    "\n",
    "        # Scheduler Step\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - \"\n",
    "            f\"Train RMSE: {train_rmse:.4f} - Val RMSE: {val_rmse:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/anaconda3/envs/dis_p1/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 1.1781 - Val Loss: 0.8106 - Train RMSE: 0.8893 - Val RMSE: 0.9010\n",
      "Epoch 2/100 - Train Loss: 0.7902 - Val Loss: 0.8084 - Train RMSE: 0.8873 - Val RMSE: 0.8998\n",
      "Epoch 3/100 - Train Loss: 0.7870 - Val Loss: 0.8060 - Train RMSE: 0.8853 - Val RMSE: 0.8985\n",
      "Epoch 4/100 - Train Loss: 0.7836 - Val Loss: 0.8038 - Train RMSE: 0.8834 - Val RMSE: 0.8972\n",
      "Epoch 5/100 - Train Loss: 0.7806 - Val Loss: 0.8015 - Train RMSE: 0.8815 - Val RMSE: 0.8960\n",
      "Epoch 6/100 - Train Loss: 0.7777 - Val Loss: 0.7993 - Train RMSE: 0.8796 - Val RMSE: 0.8947\n",
      "Epoch 7/100 - Train Loss: 0.7758 - Val Loss: 0.7971 - Train RMSE: 0.8777 - Val RMSE: 0.8935\n",
      "Epoch 8/100 - Train Loss: 0.7760 - Val Loss: 0.7943 - Train RMSE: 0.8757 - Val RMSE: 0.8919\n",
      "Epoch 9/100 - Train Loss: 0.7795 - Val Loss: 0.7891 - Train RMSE: 0.8731 - Val RMSE: 0.8890\n",
      "Epoch 10/100 - Train Loss: 0.7842 - Val Loss: 0.7756 - Train RMSE: 0.8679 - Val RMSE: 0.8814\n",
      "Epoch 11/100 - Train Loss: 0.7857 - Val Loss: 0.7447 - Train RMSE: 0.8557 - Val RMSE: 0.8637\n",
      "Epoch 12/100 - Train Loss: 0.7780 - Val Loss: 0.7162 - Train RMSE: 0.8359 - Val RMSE: 0.8470\n",
      "Epoch 13/100 - Train Loss: 0.7655 - Val Loss: 0.6946 - Train RMSE: 0.8169 - Val RMSE: 0.8346\n",
      "Epoch 14/100 - Train Loss: 0.7549 - Val Loss: 0.6852 - Train RMSE: 0.8031 - Val RMSE: 0.8290\n",
      "Epoch 15/100 - Train Loss: 0.7461 - Val Loss: 0.6759 - Train RMSE: 0.7934 - Val RMSE: 0.8232\n",
      "Epoch 16/100 - Train Loss: 0.7395 - Val Loss: 0.6725 - Train RMSE: 0.7859 - Val RMSE: 0.8210\n",
      "Epoch 17/100 - Train Loss: 0.7353 - Val Loss: 0.6692 - Train RMSE: 0.7802 - Val RMSE: 0.8188\n",
      "Epoch 18/100 - Train Loss: 0.7314 - Val Loss: 0.6652 - Train RMSE: 0.7752 - Val RMSE: 0.8166\n",
      "Epoch 19/100 - Train Loss: 0.7283 - Val Loss: 0.6656 - Train RMSE: 0.7710 - Val RMSE: 0.8169\n",
      "Epoch 20/100 - Train Loss: 0.7260 - Val Loss: 0.6617 - Train RMSE: 0.7675 - Val RMSE: 0.8147\n",
      "Epoch 21/100 - Train Loss: 0.7247 - Val Loss: 0.6623 - Train RMSE: 0.7651 - Val RMSE: 0.8150\n",
      "Epoch 22/100 - Train Loss: 0.7232 - Val Loss: 0.6588 - Train RMSE: 0.7626 - Val RMSE: 0.8123\n",
      "Epoch 23/100 - Train Loss: 0.7206 - Val Loss: 0.6592 - Train RMSE: 0.7597 - Val RMSE: 0.8132\n",
      "Epoch 24/100 - Train Loss: 0.7196 - Val Loss: 0.6601 - Train RMSE: 0.7579 - Val RMSE: 0.8134\n",
      "Epoch 25/100 - Train Loss: 0.7187 - Val Loss: 0.6613 - Train RMSE: 0.7562 - Val RMSE: 0.8147\n",
      "Epoch 26/100 - Train Loss: 0.6668 - Val Loss: 0.6499 - Train RMSE: 0.7288 - Val RMSE: 0.8069\n",
      "Epoch 27/100 - Train Loss: 0.6583 - Val Loss: 0.6480 - Train RMSE: 0.7302 - Val RMSE: 0.8059\n",
      "Epoch 28/100 - Train Loss: 0.6549 - Val Loss: 0.6487 - Train RMSE: 0.7304 - Val RMSE: 0.8063\n",
      "Epoch 29/100 - Train Loss: 0.6519 - Val Loss: 0.6519 - Train RMSE: 0.7294 - Val RMSE: 0.8078\n",
      "Epoch 30/100 - Train Loss: 0.6493 - Val Loss: 0.6532 - Train RMSE: 0.7281 - Val RMSE: 0.8088\n",
      "Epoch 31/100 - Train Loss: 0.6143 - Val Loss: 0.6463 - Train RMSE: 0.7068 - Val RMSE: 0.8048\n",
      "Epoch 32/100 - Train Loss: 0.6094 - Val Loss: 0.6455 - Train RMSE: 0.7063 - Val RMSE: 0.8042\n",
      "Epoch 33/100 - Train Loss: 0.6072 - Val Loss: 0.6452 - Train RMSE: 0.7065 - Val RMSE: 0.8040\n",
      "Epoch 34/100 - Train Loss: 0.6049 - Val Loss: 0.6448 - Train RMSE: 0.7061 - Val RMSE: 0.8039\n",
      "Epoch 35/100 - Train Loss: 0.6027 - Val Loss: 0.6456 - Train RMSE: 0.7051 - Val RMSE: 0.8044\n",
      "Epoch 36/100 - Train Loss: 0.6006 - Val Loss: 0.6453 - Train RMSE: 0.7041 - Val RMSE: 0.8041\n",
      "Epoch 37/100 - Train Loss: 0.5986 - Val Loss: 0.6437 - Train RMSE: 0.7030 - Val RMSE: 0.8033\n",
      "Epoch 38/100 - Train Loss: 0.5964 - Val Loss: 0.6435 - Train RMSE: 0.7016 - Val RMSE: 0.8032\n",
      "Epoch 39/100 - Train Loss: 0.5943 - Val Loss: 0.6437 - Train RMSE: 0.7002 - Val RMSE: 0.8032\n",
      "Epoch 40/100 - Train Loss: 0.5926 - Val Loss: 0.6440 - Train RMSE: 0.6992 - Val RMSE: 0.8033\n",
      "Epoch 41/100 - Train Loss: 0.5908 - Val Loss: 0.6453 - Train RMSE: 0.6978 - Val RMSE: 0.8042\n",
      "Epoch 42/100 - Train Loss: 0.5667 - Val Loss: 0.6432 - Train RMSE: 0.6815 - Val RMSE: 0.8027\n",
      "Epoch 43/100 - Train Loss: 0.5649 - Val Loss: 0.6426 - Train RMSE: 0.6812 - Val RMSE: 0.8024\n",
      "Epoch 44/100 - Train Loss: 0.5636 - Val Loss: 0.6426 - Train RMSE: 0.6810 - Val RMSE: 0.8024\n",
      "Epoch 45/100 - Train Loss: 0.5626 - Val Loss: 0.6429 - Train RMSE: 0.6808 - Val RMSE: 0.8026\n",
      "Epoch 46/100 - Train Loss: 0.5614 - Val Loss: 0.6426 - Train RMSE: 0.6804 - Val RMSE: 0.8024\n",
      "Epoch 47/100 - Train Loss: 0.5476 - Val Loss: 0.6424 - Train RMSE: 0.6708 - Val RMSE: 0.8023\n",
      "Epoch 48/100 - Train Loss: 0.5469 - Val Loss: 0.6423 - Train RMSE: 0.6706 - Val RMSE: 0.8022\n",
      "Epoch 49/100 - Train Loss: 0.5462 - Val Loss: 0.6423 - Train RMSE: 0.6705 - Val RMSE: 0.8022\n",
      "Epoch 50/100 - Train Loss: 0.5456 - Val Loss: 0.6422 - Train RMSE: 0.6704 - Val RMSE: 0.8021\n",
      "Epoch 51/100 - Train Loss: 0.5450 - Val Loss: 0.6422 - Train RMSE: 0.6702 - Val RMSE: 0.8022\n",
      "Epoch 52/100 - Train Loss: 0.5443 - Val Loss: 0.6421 - Train RMSE: 0.6700 - Val RMSE: 0.8021\n",
      "Epoch 53/100 - Train Loss: 0.5437 - Val Loss: 0.6422 - Train RMSE: 0.6698 - Val RMSE: 0.8022\n",
      "Epoch 54/100 - Train Loss: 0.5431 - Val Loss: 0.6419 - Train RMSE: 0.6696 - Val RMSE: 0.8020\n",
      "Epoch 55/100 - Train Loss: 0.5425 - Val Loss: 0.6420 - Train RMSE: 0.6693 - Val RMSE: 0.8021\n",
      "Epoch 56/100 - Train Loss: 0.5419 - Val Loss: 0.6421 - Train RMSE: 0.6690 - Val RMSE: 0.8022\n",
      "Epoch 57/100 - Train Loss: 0.5412 - Val Loss: 0.6421 - Train RMSE: 0.6686 - Val RMSE: 0.8021\n",
      "Epoch 58/100 - Train Loss: 0.5336 - Val Loss: 0.6421 - Train RMSE: 0.6631 - Val RMSE: 0.8021\n",
      "Epoch 59/100 - Train Loss: 0.5332 - Val Loss: 0.6420 - Train RMSE: 0.6630 - Val RMSE: 0.8021\n",
      "Epoch 60/100 - Train Loss: 0.5329 - Val Loss: 0.6419 - Train RMSE: 0.6629 - Val RMSE: 0.8020\n",
      "Epoch 61/100 - Train Loss: 0.5289 - Val Loss: 0.6419 - Train RMSE: 0.6600 - Val RMSE: 0.8020\n",
      "Epoch 62/100 - Train Loss: 0.5287 - Val Loss: 0.6419 - Train RMSE: 0.6599 - Val RMSE: 0.8020\n",
      "Epoch 63/100 - Train Loss: 0.5285 - Val Loss: 0.6419 - Train RMSE: 0.6599 - Val RMSE: 0.8020\n",
      "Epoch 64/100 - Train Loss: 0.5265 - Val Loss: 0.6419 - Train RMSE: 0.6584 - Val RMSE: 0.8020\n",
      "Epoch 65/100 - Train Loss: 0.5264 - Val Loss: 0.6419 - Train RMSE: 0.6584 - Val RMSE: 0.8020\n",
      "Epoch 66/100 - Train Loss: 0.5263 - Val Loss: 0.6419 - Train RMSE: 0.6583 - Val RMSE: 0.8020\n",
      "Epoch 67/100 - Train Loss: 0.5253 - Val Loss: 0.6419 - Train RMSE: 0.6576 - Val RMSE: 0.8020\n",
      "Epoch 68/100 - Train Loss: 0.5253 - Val Loss: 0.6419 - Train RMSE: 0.6576 - Val RMSE: 0.8020\n",
      "Epoch 69/100 - Train Loss: 0.5253 - Val Loss: 0.6419 - Train RMSE: 0.6576 - Val RMSE: 0.8020\n",
      "Epoch 70/100 - Train Loss: 0.5247 - Val Loss: 0.6419 - Train RMSE: 0.6572 - Val RMSE: 0.8020\n",
      "Epoch 71/100 - Train Loss: 0.5248 - Val Loss: 0.6419 - Train RMSE: 0.6572 - Val RMSE: 0.8020\n",
      "Epoch 72/100 - Train Loss: 0.5247 - Val Loss: 0.6419 - Train RMSE: 0.6572 - Val RMSE: 0.8020\n",
      "Epoch 73/100 - Train Loss: 0.5245 - Val Loss: 0.6419 - Train RMSE: 0.6570 - Val RMSE: 0.8020\n",
      "Epoch 74/100 - Train Loss: 0.5244 - Val Loss: 0.6419 - Train RMSE: 0.6570 - Val RMSE: 0.8020\n",
      "Epoch 75/100 - Train Loss: 0.5244 - Val Loss: 0.6419 - Train RMSE: 0.6570 - Val RMSE: 0.8020\n",
      "Epoch 76/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 77/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 78/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 79/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 80/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 81/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 82/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 83/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 84/100 - Train Loss: 0.5243 - Val Loss: 0.6419 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 85/100 - Train Loss: 0.5242 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 86/100 - Train Loss: 0.5242 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 87/100 - Train Loss: 0.5244 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 88/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 89/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 90/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8020\n",
      "Epoch 91/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8019\n",
      "Epoch 92/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8019\n",
      "Epoch 93/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6569 - Val RMSE: 0.8019\n",
      "Epoch 94/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6570 - Val RMSE: 0.8019\n",
      "Epoch 95/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6570 - Val RMSE: 0.8019\n",
      "Epoch 96/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6570 - Val RMSE: 0.8019\n",
      "Epoch 97/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6570 - Val RMSE: 0.8019\n",
      "Epoch 98/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6570 - Val RMSE: 0.8019\n",
      "Epoch 99/100 - Train Loss: 0.5243 - Val Loss: 0.6418 - Train RMSE: 0.6570 - Val RMSE: 0.8019\n",
      "Epoch 100/100 - Train Loss: 0.5242 - Val Loss: 0.6418 - Train RMSE: 0.6570 - Val RMSE: 0.8019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443173/2148372479.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path))\n"
     ]
    }
   ],
   "source": [
    "# Use all data for low-interaction model\n",
    "train_dataset = RatingsDataset(train_data)  # Entire training data\n",
    "val_dataset = RatingsDataset(val_data)  # Entire validation data\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "model = MatrixFactorization(n_users, n_books, EMBED_DIM).to(DEVICE)\n",
    "# Define optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Train the low-interaction model\n",
    "model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    lambda pred, target, model, reg_lambda: loss_fn(pred, target, model, reg_lambda, loss_type=\"MSE\"),\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    PATIENCE,\n",
    "    \"model.pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import joblib\n",
    "\n",
    "# SEED = 42\n",
    "# optuna.seed = SEED\n",
    "# def objective(trial):\n",
    "#     # Hyperparameter Search Space\n",
    "#     emb_dim = trial.suggest_int(\"EMBED_DIM\", 32, 128, step=16)\n",
    "#     lr = trial.suggest_loguniform(\"LR\", 1e-5, 1e-3)\n",
    "#     reg_lambda = trial.suggest_loguniform(\"REG_LAMBDA\", 1e-5, 1e-1)\n",
    "#     patience = trial.suggest_int(\"PATIENCE\", 5, 20)\n",
    "    \n",
    "#     # Define Model, Optimizer, and Scheduler\n",
    "#     model = MatrixFactorization(n_users, n_books, emb_dim).to(DEVICE)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode=\"min\", factor=0.5, patience=2, verbose=False\n",
    "#     )\n",
    "    \n",
    "#     # Train Model\n",
    "#     model = train_model(\n",
    "#         model,\n",
    "#         train_loader,  # Use pre-defined loaders\n",
    "#         val_loader,    # Use pre-defined loaders\n",
    "#         lambda pred, target, model, reg_lambda: loss_fn(pred, target, model, reg_lambda, loss_type=\"MSE\"),\n",
    "#         optimizer,\n",
    "#         scheduler,\n",
    "#         EPOCHS,\n",
    "#         patience,\n",
    "#         \"model_trial.pt\",\n",
    "#     )\n",
    "    \n",
    "#     # Evaluate on Validation Set\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0\n",
    "#     val_squared_error = 0\n",
    "#     num_val_samples = 0\n",
    "#     with torch.no_grad():\n",
    "#         for users, items, ratings in val_loader:\n",
    "#             users = users.to(DEVICE)\n",
    "#             items = items.to(DEVICE)\n",
    "#             ratings = ratings.to(DEVICE)\n",
    "#             preds = model(users, items, offsets_tensor)\n",
    "#             val_loss = loss_fn(preds, ratings, model, 0)  # No regularization\n",
    "#             total_val_loss += val_loss.item()\n",
    "#             val_squared_error += ((preds - ratings) ** 2).sum().item()\n",
    "#             num_val_samples += ratings.size(0)\n",
    "#     val_rmse = np.sqrt(val_squared_error / num_val_samples)\n",
    "    \n",
    "#     return val_rmse\n",
    "\n",
    "# # Create dataloaders \n",
    "\n",
    "\n",
    "# # Create Optuna Study\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# # Save the Optuna Study\n",
    "# joblib.dump(study, \"optuna_study.pkl\")\n",
    "\n",
    "# # Retrieve the Best Trial\n",
    "# best_trial = study.best_trial\n",
    "# print(\"Best hyperparameters:\", best_trial.params)\n",
    "\n",
    "# # Train the Final Model with Best Hyperparameters\n",
    "# best_params = best_trial.params\n",
    "# final_model = MatrixFactorization(n_users, n_books, best_params[\"EMBED_DIM\"]).to(DEVICE)\n",
    "# final_optimizer = optim.Adam(final_model.parameters(), lr=best_params[\"LR\"])\n",
    "# final_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     final_optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True\n",
    "# )\n",
    "\n",
    "# final_model = train_model(\n",
    "#     final_model,\n",
    "#     train_loader,  # Use pre-defined loaders\n",
    "#     val_loader,    # Use pre-defined loaders\n",
    "#     lambda pred, target, model, reg_lambda: loss_fn(pred, target, model, best_params[\"REG_LAMBDA\"], loss_type=\"MSE\"),\n",
    "#     final_optimizer,\n",
    "#     final_scheduler,\n",
    "#     EPOCHS,\n",
    "#     best_params[\"PATIENCE\"],\n",
    "#     \"final_best_model.pt\",\n",
    "# )\n",
    "\n",
    "# # Save the Final Model\n",
    "# torch.save(final_model.state_dict(), \"final_best_model_state_dict.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Interaction User Model RMSE: 0.9888\n"
     ]
    }
   ],
   "source": [
    "# now see it for the low interaction users \n",
    "low_int_usr = val_data[val_data['user_idx'].isin(low_interaction_users)]\n",
    "low_int_usr_dataset = RatingsDataset(low_int_usr)\n",
    "low_int_usr_loader = DataLoader(\n",
    "    low_int_usr_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_error_sq = 0\n",
    "    for users, items, ratings in low_int_usr_loader:\n",
    "        users = users.to(DEVICE)\n",
    "        items = items.to(DEVICE)\n",
    "        ratings = ratings.to(DEVICE)\n",
    "\n",
    "        preds = model(users, items, offsets_tensor)\n",
    "        val_loss = loss_fn(preds, ratings, model, 0)  # No regularization during validation\n",
    "        total_val_loss += val_loss.item()\n",
    "        total_error_sq += ((preds - ratings) ** 2).sum().item()\n",
    "\n",
    "val_rmse = np.sqrt(total_error_sq / len(low_int_usr_loader.dataset))\n",
    "print(f\"Low Interaction User Model RMSE: {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Interaction User Model RMSE: 0.7584\n"
     ]
    }
   ],
   "source": [
    "# now for the high interaction users\n",
    "high_int_usr = val_data[~val_data['user_idx'].isin(low_interaction_users)]\n",
    "high_int_usr_dataset = RatingsDataset(high_int_usr)\n",
    "high_int_usr_loader = DataLoader(\n",
    "    high_int_usr_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for users, items, ratings in high_int_usr_loader:\n",
    "        users = users.to(DEVICE)\n",
    "        items = items.to(DEVICE)\n",
    "        ratings = ratings.to(DEVICE)\n",
    "\n",
    "        preds = model(users, items, offsets_tensor)\n",
    "        val_loss = loss_fn(preds, ratings, model, 0)  # No regularization during validation\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "val_rmse = np.sqrt(total_val_loss / len(high_int_usr_loader))\n",
    "print(f\"High Interaction User Model RMSE: {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created submission.csv\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the test set\n",
    "# Map user and book IDs to indices\n",
    "test_df['user_idx'] = test_df['user_id'].map(user2idx)\n",
    "test_df['book_idx'] = test_df['book_id'].map(book2idx)\n",
    "\n",
    "# Handle unknown users/items by assigning them to baseline indices\n",
    "test_df[\"user_idx\"] = test_df[\"user_idx\"].fillna(n_users - 1).astype(int)\n",
    "test_df[\"book_idx\"] = test_df[\"book_idx\"].fillna(n_books - 1).astype(int)\n",
    "\n",
    "# Prepare tensors for test users and items\n",
    "test_users = torch.tensor(test_df[\"user_idx\"].values, dtype=torch.long).to(DEVICE)\n",
    "test_items = torch.tensor(test_df[\"book_idx\"].values, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "# Predict using low-interaction model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = model(test_users, test_items, offsets_tensor).cpu().numpy()\n",
    "\n",
    "# Clip predictions to range (1 to 5)\n",
    "test_preds = np.clip(test_preds, 1.0, 5.0)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\"id\": test_df[\"id\"], \"rating\": test_preds})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Created submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['rating'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rating'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dis_p1/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rating'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_df_filtered \u001b[38;5;241m=\u001b[39m test_df[test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(users_with_less_than_5_reviews)]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plot the ratings on hist\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtest_df_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhist()\n",
      "File \u001b[0;32m~/anaconda3/envs/dis_p1/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/dis_p1/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rating'"
     ]
    }
   ],
   "source": [
    "user_counts = train_df[\"user_id\"].value_counts()\n",
    "users_with_less_than_5_reviews = user_counts[user_counts < 55].index\n",
    "\n",
    "# get the test_df rows with user_ids in users_with_less_than_5_reviews\n",
    "test_df_filtered = test_df[test_df[\"user_id\"].isin(users_with_less_than_5_reviews)]\n",
    "\n",
    "# plot the ratings on hist\n",
    "test_df_filtered[\"rating\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "2.0    21973\n",
       "1.0    17689\n",
       "1.5    15399\n",
       "2.5    14177\n",
       "3.0     9863\n",
       "3.5     8167\n",
       "4.0     6753\n",
       "5.0     5577\n",
       "4.5      925\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"rating\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the rating hist in train_df for only the users with less than X (X will be adjustable) reviews\n",
    "train_df[train_df[\"user_id\"].isin(user_counts[user_counts < 55545].index)][\n",
    "    \"rating\"\n",
    "].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1       3.910462\n",
       "2       3.813296\n",
       "3       2.748508\n",
       "4       2.234422\n",
       "5       2.169215\n",
       "          ...   \n",
       "239     2.054393\n",
       "311     2.172026\n",
       "315     1.450794\n",
       "472     1.580508\n",
       "1837    2.210942\n",
       "Name: rating, Length: 123, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id_counts = train_data.groupby(\"user_id\")[\"user_id\"].transform(\"size\")\n",
    "count2mean = train_data.groupby(user_id_counts)[\"rating\"].mean()\n",
    "count2mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing books in test not in train: 0\n",
      "Book counts:\n",
      "book_id\n",
      "408       257\n",
      "748       213\n",
      "522       149\n",
      "356       142\n",
      "26        142\n",
      "         ... \n",
      "247693      1\n",
      "248107      1\n",
      "245643      1\n",
      "246570      1\n",
      "246356      1\n",
      "Name: count, Length: 15712, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "train_data = pd.read_csv(\"../data/train.csv\")\n",
    "test_books = set(test_df[\"book_id\"].unique())\n",
    "train_books = set(train_data[\"book_id\"].unique())\n",
    "\n",
    "missing_books = test_books - train_books\n",
    "print(f\"Number of missing books in test not in train: {len(missing_books)}\")\n",
    "if len(missing_books) > 0:\n",
    "    print(\"Missing books:\", missing_books)\n",
    "\n",
    "\n",
    "# print number of occurrences of each book in the train data\n",
    "book_counts = train_data[\"book_id\"].value_counts()\n",
    "print(\"Book counts:\")\n",
    "print(book_counts)\n",
    "\n",
    "# show the occurences histogram in log scale\n",
    "book_counts.hist(bins=10, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book counts sorted:\n",
      "book_id\n",
      "408       257\n",
      "748       213\n",
      "522       149\n",
      "356       142\n",
      "26        142\n",
      "         ... \n",
      "245793      1\n",
      "249072      1\n",
      "248519      1\n",
      "66651       1\n",
      "249242      1\n",
      "Name: count, Length: 15712, dtype: int64\n",
      "Top books:\n",
      "book_id\n",
      "408     257\n",
      "748     213\n",
      "522     149\n",
      "356     142\n",
      "26      142\n",
      "       ... \n",
      "5873     58\n",
      "2527     57\n",
      "2488     57\n",
      "516      56\n",
      "1051     56\n",
      "Name: count, Length: 100, dtype: int64\n",
      "Top books ratings:\n",
      "book_id\n",
      "18        2.256944\n",
      "26        3.915493\n",
      "28        2.065789\n",
      "37        1.453271\n",
      "67        1.898810\n",
      "            ...   \n",
      "5873      1.974138\n",
      "5887      1.768116\n",
      "6330      1.427536\n",
      "6933      1.406780\n",
      "243217    4.206349\n",
      "Name: rating, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# sort the rating of the books by the number of occurrences\n",
    "book_counts_sorted = book_counts.sort_values(ascending=False)\n",
    "print(\"Book counts sorted:\")\n",
    "print(book_counts_sorted)\n",
    "\n",
    "# show the top 10 books with the most occurrences\n",
    "top_books = book_counts_sorted.head(100)\n",
    "print(\"Top books:\")\n",
    "print(top_books)\n",
    "\n",
    "# get the each avg rating of the top books\n",
    "top_books_ratings = (\n",
    "    train_data[train_data[\"book_id\"].isin(top_books.index)]\n",
    "    .groupby(\"book_id\")[\"rating\"]\n",
    "    .mean()\n",
    ")\n",
    "print(\"Top books ratings:\")\n",
    "print(top_books_ratings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
