{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Works with OpenLibrary API, we want to improve it with google books api'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Works with OpenLibrary API, we want to improve it with google books api\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"../data/books_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0440234743</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0452264464</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN  book_id\n",
       "0  0002005018        1\n",
       "1  0374157065        3\n",
       "2  0399135782        5\n",
       "3  0440234743       18\n",
       "4  0452264464       19"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from requests import Session\n",
    "# from requests_ratelimiter import LimiterAdapter\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def fetch_book_data():\n",
    "#     # Set up rate-limited session\n",
    "#     session = Session()\n",
    "#     adapter = LimiterAdapter(per_second=5)  # Adjust rate limit as needed\n",
    "#     session.mount(\"https://openlibrary.org/\", adapter)\n",
    "\n",
    "#     # Read input data\n",
    "#     books_df = pd.read_csv('../data/books.csv')\n",
    "\n",
    "#     extended_data = []\n",
    "#     for isbn in tqdm(books_df['ISBN'], desc=\"Fetching book data\"):\n",
    "#         try:\n",
    "#             # Fetch book data\n",
    "#             response = session.get(f\"https://openlibrary.org/isbn/{isbn}.json\")\n",
    "#             if response.status_code == 200:\n",
    "#                 book_data = response.json()\n",
    "\n",
    "#                 # Extract features\n",
    "#                 features = {\n",
    "#                     'ISBN': isbn,\n",
    "#                     'number_of_pages': book_data.get('number_of_pages'),\n",
    "#                     'genres': ','.join(book_data.get('genres', [])),\n",
    "#                     'publish_date': book_data.get('publish_date'),\n",
    "#                     'authors': ','.join([author.get('key', '') for author in book_data.get('authors', [])]),\n",
    "#                     'publishers': ','.join(book_data.get('publishers', [])),\n",
    "#                     'languages': ','.join([language.get('key', '') for language in book_data.get('languages', [])]),\n",
    "#                     'subjects': ','.join(book_data.get('subjects', []))\n",
    "#                 }\n",
    "#                 extended_data.append(features)\n",
    "#             else:\n",
    "#                 extended_data.append({'ISBN': isbn})\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with ISBN {isbn}: {e}\")\n",
    "#             extended_data.append({'ISBN': isbn})\n",
    "\n",
    "#     extended_df = pd.DataFrame(extended_data)\n",
    "#     books_df = books_df.merge(extended_df, on='ISBN', how='left')\n",
    "#     books_df.to_csv('../data/extended_books.csv', index=False)\n",
    "#     print(\"Extended dataset created and saved.\")\n",
    "\n",
    "#     session.close()\n",
    "\n",
    "# fetch_book_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching book data:   0%|          | 0/16599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching book data:  51%|█████▏    | 8521/16599 [2:42:03<90:46:03, 40.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with ISBN 0140324623: HTTPSConnectionPool(host='openlibrary.org', port=443): Max retries exceeded with url: /books/OL22474773M.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f6f16adc230>, 'Connection to openlibrary.org timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching book data:  99%|█████████▉| 16455/16599 [4:55:55<1:34:57, 39.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with ISBN 0671449028: HTTPSConnectionPool(host='openlibrary.org', port=443): Max retries exceeded with url: /isbn/0671449028.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f6f16b14290>, 'Connection to openlibrary.org timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching book data: 100%|██████████| 16599/16599 [4:58:10<00:00,  1.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from requests import Session\n",
    "# from requests_ratelimiter import LimiterAdapter\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def fetch_author_name(session, author_key):\n",
    "#     author_url = f\"https://openlibrary.org{author_key}.json\"\n",
    "#     response = session.get(author_url)\n",
    "#     if response.status_code == 200:\n",
    "#         author_data = response.json()\n",
    "#         return author_data.get(\"name\")\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def fetch_book_data():\n",
    "#     session = Session()\n",
    "#     adapter = LimiterAdapter(per_second=5)\n",
    "#     session.mount(\"https://openlibrary.org/\", adapter)\n",
    "\n",
    "#     books_df = pd.read_csv(\"../data/books_fixed.csv\")\n",
    "\n",
    "#     extended_data = []\n",
    "#     for isbn in tqdm(books_df[\"ISBN\"], desc=\"Fetching book data\"):\n",
    "#         try:\n",
    "#             response = session.get(f\"https://openlibrary.org/isbn/{isbn}.json\")\n",
    "#             if response.status_code == 200:\n",
    "#                 book_data = response.json()\n",
    "\n",
    "#                 authors = [\n",
    "#                     fetch_author_name(session, author.get(\"key\"))\n",
    "#                     for author in book_data.get(\"authors\", [])\n",
    "#                 ]\n",
    "\n",
    "#                 features = {\n",
    "#                     \"ISBN\": isbn,\n",
    "#                     \"number_of_pages\": book_data.get(\"number_of_pages\"),\n",
    "#                     \"genres\": \",\".join(book_data.get(\"genres\", [])),\n",
    "#                     \"publish_date\": book_data.get(\"publish_date\"),\n",
    "#                     \"authors\": \",\".join(\n",
    "#                         filter(None, authors)\n",
    "#                     ),  # Join only non-None authors\n",
    "#                     \"publishers\": \",\".join(book_data.get(\"publishers\", [])),\n",
    "#                     \"languages\": \",\".join(\n",
    "#                         [\n",
    "#                             language.get(\"key\", \"\").replace(\"/languages/\", \"\")\n",
    "#                             for language in book_data.get(\"languages\", [])\n",
    "#                         ]\n",
    "#                     ),\n",
    "#                     \"subjects\": \",\".join(book_data.get(\"subjects\", [])),\n",
    "#                 }\n",
    "#                 extended_data.append(features)\n",
    "#             else:\n",
    "#                 extended_data.append({\"ISBN\": isbn})\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with ISBN {isbn}: {e}\")\n",
    "#             extended_data.append({\"ISBN\": isbn})\n",
    "\n",
    "#     extended_df = pd.DataFrame(extended_data)\n",
    "#     books_df = books_df.merge(extended_df, on=\"ISBN\", how=\"left\")\n",
    "#     books_df.to_csv(\"../data/extended_books_openlibrary.csv\", index=False)\n",
    "#     print(\"Extended dataset created and saved.\")\n",
    "\n",
    "#     session.close()\n",
    "\n",
    "\n",
    "# fetch_book_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching book data:   0%|          | 0/8494 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching book data: 100%|██████████| 8494/8494 [4:23:29<00:00,  1.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended dataset created and saved with Google Books API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Google Books API\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# from requests import Session\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import os\n",
    "\n",
    "\n",
    "# def fetch_book_data_google():\n",
    "#     session = Session()\n",
    "#     books_df = pd.read_csv(\"../data/books_fixed2.csv\")\n",
    "\n",
    "#     extended_books_path = \"../data/extended_books_google2.csv\"\n",
    "\n",
    "#     if os.path.exists(extended_books_path):\n",
    "#         extended_books_df = pd.read_csv(extended_books_path)\n",
    "#         fetched_isbns = set(extended_books_df[\"ISBN\"].dropna())\n",
    "#     else:\n",
    "#         extended_books_df = pd.DataFrame()\n",
    "#         fetched_isbns = set()\n",
    "\n",
    "#     new_isbns = books_df[~books_df[\"ISBN\"].isin(fetched_isbns)][\"ISBN\"]\n",
    "\n",
    "#     extended_data = []\n",
    "#     for isbn in tqdm(new_isbns, desc=\"Fetching book data\"):\n",
    "#         time.sleep(1.5)\n",
    "#         try:\n",
    "#             response = session.get(\n",
    "#                 f\"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}\"\n",
    "#             )\n",
    "#             if response.status_code == 200:\n",
    "#                 results = response.json()\n",
    "#                 if results[\"totalItems\"] > 0:\n",
    "#                     book_data = results[\"items\"][0][\"volumeInfo\"]\n",
    "\n",
    "#                     # Extract relevant information\n",
    "#                     features = {\n",
    "#                         \"ISBN\": isbn,\n",
    "#                         \"title\": book_data.get(\"title\"),\n",
    "#                         \"subtitle\": book_data.get(\"subtitle\"),\n",
    "#                         \"authors\": \",\".join(book_data.get(\"authors\", [])),\n",
    "#                         \"publisher\": book_data.get(\"publisher\"),\n",
    "#                         \"publishedDate\": book_data.get(\"publishedDate\"),\n",
    "#                         \"description\": book_data.get(\"description\"),\n",
    "#                         \"pageCount\": book_data.get(\"pageCount\"),\n",
    "#                         \"maturityRating\": book_data.get(\"maturityRating\"),\n",
    "#                         \"language\": book_data.get(\"language\"),\n",
    "#                         \"categories\": \",\".join(book_data.get(\"categories\", [])),\n",
    "#                         \"ratingsCount\": book_data.get(\"ratingsCount\"),\n",
    "#                         \"averageRating\": book_data.get(\"averageRating\"),\n",
    "#                         \"textSnippet\": book_data.get(\"searchInfo\", {}).get(\n",
    "#                             \"textSnippet\"\n",
    "#                         ),\n",
    "#                     }\n",
    "#                     extended_data.append(features)\n",
    "#                 else:\n",
    "#                     extended_data.append({\"ISBN\": isbn})\n",
    "#             else:\n",
    "#                 print(f\"Failed to fetch data for ISBN {isbn}\")\n",
    "#                 extended_data.append({\"ISBN\": isbn})\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with ISBN {isbn}: {e}\")\n",
    "#             extended_data.append({\"ISBN\": isbn})\n",
    "\n",
    "#     new_extended_df = pd.DataFrame(extended_data)\n",
    "\n",
    "#     if not extended_books_df.empty:\n",
    "#         combined_df = pd.concat([extended_books_df, new_extended_df], ignore_index=True)\n",
    "#     else:\n",
    "#         combined_df = new_extended_df\n",
    "\n",
    "#     combined_df.to_csv(extended_books_path, index=False)\n",
    "#     print(\"Extended dataset created and saved with Google Books API.\")\n",
    "\n",
    "#     session.close()\n",
    "\n",
    "\n",
    "# fetch_book_data_google()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "books = pd.read_csv(\"../data/extended_books_google.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop textSnippet column its missing all entries\n",
    "books = books.drop(columns=[\"textSnippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN               0.000000\n",
       "title             21.585638\n",
       "subtitle          76.378095\n",
       "authors           22.073619\n",
       "publisher         30.411471\n",
       "publishedDate     21.657931\n",
       "description       23.176095\n",
       "pageCount         21.802518\n",
       "maturityRating    21.585638\n",
       "language          21.585638\n",
       "categories        22.459184\n",
       "ratingsCount      73.221278\n",
       "averageRating     73.221278\n",
       "dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see missing data per column in %\n",
    "books.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature mask for missing data (pageCount, description, categories)\n",
    "books[\"pageCount_missing\"] = books[\"pageCount\"].isnull()\n",
    "books[\"description_missing\"] = books[\"description\"].isnull()\n",
    "books[\"categories_missing\"] = books[\"categories\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing pageCount with knnImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "books[\"pageCount\"] = imputer.fit_transform(books[[\"pageCount\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averageRating and ratingsCount are missing for some books, fill with 0 and add missing data mask\n",
    "books[\"averageRating_missing\"] = books[\"averageRating\"].isnull()\n",
    "books[\"ratingsCount_missing\"] = books[\"ratingsCount\"].isnull()\n",
    "\n",
    "# fill with KNNImputer\n",
    "books[\"averageRating\"] = imputer.fit_transform(books[[\"averageRating\"]])\n",
    "books[\"ratingsCount\"] = imputer.fit_transform(books[[\"ratingsCount\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge title subtitle and description into one column. If one is missing just use \"\"\n",
    "books[\"full_text\"] = (\n",
    "    books[\"title\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + books[\"subtitle\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + books[\"description\"]\n",
    ")\n",
    "\n",
    "# drop\n",
    "books = books.drop(columns=[\"title\", \"subtitle\", \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year from publishedDate\n",
    "books[\"publishedDate\"] = pd.to_datetime(books[\"publishedDate\"], errors=\"coerce\")\n",
    "books[\"publishedYear\"] = books[\"publishedDate\"].dt.year\n",
    "\n",
    "# drop publishedDate\n",
    "books = books.drop(columns=[\"publishedDate\"])\n",
    "\n",
    "# fill missing publishedYear with median\n",
    "books[\"publishedYear\"] = books[\"publishedYear\"].fillna(books[\"publishedYear\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask missing data\n",
    "books[\"publisher_missing\"] = books[\"publisher\"].isnull()\n",
    "books[\"publisher\"] = books[\"publisher\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill maturityRating with most frequent value\n",
    "books[\"maturityRating\"] = books[\"maturityRating\"].fillna(\n",
    "    books[\"maturityRating\"].mode().values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill language with most frequent value\n",
    "books[\"language\"] = books[\"language\"].fillna(books[\"language\"].mode().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing full_text with \"\"\n",
    "books[\"full_text\"] = books[\"full_text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle authors\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_author_vocab(authors_list, pad_token=\"<pad>\", unk_token=\"<unk>\"):\n",
    "    all_authors = [author for authors in authors_list for author in authors if author]\n",
    "    author_counter = Counter(all_authors)\n",
    "    author_vocab = {pad_token: 0, unk_token: 1}  # Start with special tokens\n",
    "    for index, author in enumerate(author_counter.keys(), start=2):\n",
    "        author_vocab[author] = index\n",
    "\n",
    "    return author_vocab\n",
    "\n",
    "\n",
    "def get_first_author(x):\n",
    "    if isinstance(x, str):\n",
    "        x = [x]\n",
    "    elif isinstance(x, list) and len(x) > 0:\n",
    "        x = [x[0]]\n",
    "    else:\n",
    "        x = [\"<pad>\"]\n",
    "    return x\n",
    "\n",
    "\n",
    "books[\"authors\"] = books[\"authors\"].fillna(\"[]\").apply(get_first_author)\n",
    "author_vocab = build_author_vocab(books[\"authors\"])\n",
    "\n",
    "\n",
    "def encode_first_author(authors, vocab):\n",
    "    return vocab.get(authors[0], vocab[\"<unk>\"])\n",
    "\n",
    "\n",
    "books[\"author_label\"] = books[\"authors\"].apply(\n",
    "    lambda authors: encode_first_author(authors, author_vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stef/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# handle categories\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import ast\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess_categories(category_list):\n",
    "    if isinstance(category_list, str):\n",
    "        try:\n",
    "            # Safely convert string representations of lists to actual lists\n",
    "            category_list = ast.literal_eval(category_list)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If it's a plain string, wrap it in a list\n",
    "            category_list = [category_list]\n",
    "\n",
    "    # Remove punctuation and split into words\n",
    "    processed_categories = []\n",
    "    for category in category_list:\n",
    "        # Remove punctuation\n",
    "        category = category.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        # Split into words and apply stemming\n",
    "        words = category.split()\n",
    "        stemmed_words = [\n",
    "            stemmer.stem(word.lower()) for word in words\n",
    "        ]  # Convert to lowercase and stem\n",
    "        processed_categories.extend(stemmed_words)\n",
    "\n",
    "    return processed_categories\n",
    "\n",
    "\n",
    "# Apply the preprocessing function safely\n",
    "books[\"categories\"] = books[\"categories\"].fillna(\"[]\").apply(preprocess_categories)\n",
    "\n",
    "\n",
    "def build_category_vocab(categories_list, pad_token=\"<pad>\", unk_token=\"<unk>\"):\n",
    "    # Flatten the list of lists to count all unique category words\n",
    "    all_categories = [word for categories in categories_list for word in categories]\n",
    "\n",
    "    # Use Counter to count the frequency of each word\n",
    "    category_counter = Counter(all_categories)\n",
    "\n",
    "    category_vocab = {pad_token: 0, unk_token: 1}\n",
    "    for index, word in enumerate(category_counter.keys(), start=2):\n",
    "        category_vocab[word] = index\n",
    "\n",
    "    return category_vocab\n",
    "\n",
    "\n",
    "category_vocab = build_category_vocab(books[\"categories\"])\n",
    "\n",
    "\n",
    "def encode_and_pad_categories(categories, vocab, max_len, pad_token=\"<pad>\"):\n",
    "    pad_index = vocab[pad_token]\n",
    "    encoded_categories = [vocab.get(word, vocab[\"<unk>\"]) for word in categories]\n",
    "\n",
    "    if len(encoded_categories) < max_len:\n",
    "        encoded_categories += [pad_index] * (max_len - len(encoded_categories))\n",
    "    else:\n",
    "        encoded_categories = encoded_categories[:max_len]\n",
    "\n",
    "    return encoded_categories\n",
    "\n",
    "\n",
    "max_categories_len = books[\"categories\"].apply(len).max()\n",
    "\n",
    "books[\"category_label\"] = books[\"categories\"].apply(\n",
    "    lambda categories: encode_and_pad_categories(\n",
    "        categories, category_vocab, max_categories_len\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories\n",
       "[fiction]                     7487\n",
       "[]                            3728\n",
       "[juvenil, fiction]            1020\n",
       "[biographi, autobiographi]     638\n",
       "[comic, graphic, novel]        146\n",
       "                              ... \n",
       "[artifici, intellig]             1\n",
       "[sex]                            1\n",
       "[chang, psycholog]               1\n",
       "[love, stori, american]          1\n",
       "[crime, investig]                1\n",
       "Name: count, Length: 947, dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"categories\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN                     0.0\n",
       "authors                  0.0\n",
       "publisher                0.0\n",
       "pageCount                0.0\n",
       "maturityRating           0.0\n",
       "language                 0.0\n",
       "categories               0.0\n",
       "ratingsCount             0.0\n",
       "averageRating            0.0\n",
       "pageCount_missing        0.0\n",
       "description_missing      0.0\n",
       "categories_missing       0.0\n",
       "averageRating_missing    0.0\n",
       "ratingsCount_missing     0.0\n",
       "full_text                0.0\n",
       "publishedYear            0.0\n",
       "publisher_missing        0.0\n",
       "author_label             0.0\n",
       "category_label           0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chekc missing in %\n",
    "books.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stef/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# import numpy as np\n",
    "# import string\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from collections import Counter\n",
    "# import ast\n",
    "# from nltk.stem import PorterStemmer\n",
    "# import nltk\n",
    "# from sklearn.preprocessing import LabelEncoder  # , MultiLabelBinarizer\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# class BooksDataset(Dataset):\n",
    "#     def __init__(self, file_path, n_neighbors=5):\n",
    "#         self.books = pd.read_csv(file_path)\n",
    "#         # self.books = self.books.drop(columns=[\"textSnippet\"])\n",
    "\n",
    "#         self.imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "#         self.stemmer = PorterStemmer()\n",
    "\n",
    "#         self._preprocess()\n",
    "\n",
    "#     def _preprocess(self):\n",
    "#         # create masks for missing data\n",
    "#         self.books[\"pageCount_missing\"] = self.books[\"pageCount\"].isnull().astype(int)\n",
    "#         self.books[\"description_missing\"] = (\n",
    "#             self.books[\"description\"].isnull().astype(int)\n",
    "#         )\n",
    "#         self.books[\"categories_missing\"] = self.books[\"categories\"].isnull().astype(int)\n",
    "#         self.books[\"averageRating_missing\"] = (\n",
    "#             self.books[\"averageRating\"].isnull().astype(int)\n",
    "#         )\n",
    "#         self.books[\"ratingsCount_missing\"] = (\n",
    "#             self.books[\"ratingsCount\"].isnull().astype(int)\n",
    "#         )\n",
    "#         self.books[\"publisher_missing\"] = self.books[\"publisher\"].isnull().astype(int)\n",
    "\n",
    "#         # Handle missing numerical values\n",
    "#         self.books[\"pageCount\"] = self.imputer.fit_transform(self.books[[\"pageCount\"]])\n",
    "#         self.books[\"averageRating\"] = self.imputer.fit_transform(\n",
    "#             self.books[[\"averageRating\"]]\n",
    "#         )\n",
    "#         self.books[\"ratingsCount\"] = self.imputer.fit_transform(\n",
    "#             self.books[[\"ratingsCount\"]]\n",
    "#         )\n",
    "\n",
    "#         self.books[\"full_text_embeddings\"] = self.books[\"full_text_embeddings\"].apply(\n",
    "#             eval\n",
    "#         )\n",
    "#         self.books = self.books.drop(columns=[\"title\", \"subtitle\", \"description\"])\n",
    "\n",
    "#         # Handle dates\n",
    "#         self.books[\"publishedDate\"] = pd.to_datetime(\n",
    "#             self.books[\"publishedDate\"], errors=\"coerce\"\n",
    "#         )\n",
    "#         self.books[\"publishedYear\"] = self.books[\"publishedDate\"].dt.year\n",
    "#         self.books[\"publishedYear\"] = self.books[\"publishedYear\"].fillna(\n",
    "#             self.books[\"publishedYear\"].median()\n",
    "#         )\n",
    "#         self.books = self.books.drop(columns=[\"publishedDate\"])\n",
    "\n",
    "#         # Fill missing categorical values\n",
    "#         # fill missing publisher with \"Unknown\" and convert to numerical\n",
    "#         self.books[\"publisher\"] = self.books[\"publisher\"].fillna(\"Unknown\")\n",
    "#         pub_enc = LabelEncoder()\n",
    "#         self.books[\"publisher\"] = pub_enc.fit_transform(self.books[\"publisher\"])\n",
    "\n",
    "#         # fill missing maturityRating with most frequent value and convert to numerical\n",
    "#         self.books[\"maturityRating\"] = self.books[\"maturityRating\"].fillna(\n",
    "#             self.books[\"maturityRating\"].mode().values[0]\n",
    "#         )\n",
    "#         self.books[\"maturityRating\"] = (\n",
    "#             self.books[\"maturityRating\"].astype(\"category\").cat.codes\n",
    "#         )\n",
    "\n",
    "#         # fill missing language with most frequent value and convert to numerical\n",
    "#         self.books[\"language\"] = self.books[\"language\"].fillna(\n",
    "#             self.books[\"language\"].mode().values[0]\n",
    "#         )\n",
    "#         lang_enc = LabelEncoder()\n",
    "#         self.books[\"language\"] = lang_enc.fit_transform(self.books[\"language\"])\n",
    "\n",
    "#         self.books[\"full_text\"] = self.books[\"full_text\"].fillna(\"\")\n",
    "\n",
    "#         # Handle authors: Keep only the first author\n",
    "#         self.books[\"authors\"] = (\n",
    "#             self.books[\"authors\"].fillna(\"[]\").apply(self._get_first_author)\n",
    "#         )\n",
    "#         self.author_vocab = self._build_author_vocab(self.books[\"authors\"])\n",
    "#         self.books[\"author_label\"] = self.books[\"authors\"].apply(\n",
    "#             lambda authors: self.author_vocab.get(\n",
    "#                 authors[0], self.author_vocab[\"<unk>\"]\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         # Handle categories: Remove punctuation, split, and stem\n",
    "#         self.books[\"categories\"] = (\n",
    "#             self.books[\"categories\"].fillna(\"[]\").apply(self._preprocess_categories)\n",
    "#         )\n",
    "#         self.category_vocab = self._build_category_vocab(self.books[\"categories\"])\n",
    "#         self.max_categories_len = self.books[\"categories\"].apply(len).max()\n",
    "#         self.books[\"category_label\"] = self.books[\"categories\"].apply(\n",
    "#             lambda categories: self._encode_and_pad_categories(\n",
    "#                 categories, self.max_categories_len\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     def _get_first_author(self, x):\n",
    "#         if isinstance(x, str):\n",
    "#             x = [x]\n",
    "#         elif isinstance(x, list) and len(x) > 0:\n",
    "#             x = [x[0]]\n",
    "#         else:\n",
    "#             x = [\"<pad>\"]\n",
    "#         return x\n",
    "\n",
    "#     def _build_author_vocab(self, authors_list):\n",
    "#         all_authors = [\n",
    "#             author for authors in authors_list for author in authors if author\n",
    "#         ]\n",
    "#         author_counter = Counter(all_authors)\n",
    "#         author_vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "#         for index, author in enumerate(author_counter.keys(), start=2):\n",
    "#             author_vocab[author] = index\n",
    "#         return author_vocab\n",
    "\n",
    "#     def _preprocess_categories(self, category_list):\n",
    "#         if isinstance(category_list, str):\n",
    "#             try:\n",
    "#                 category_list = ast.literal_eval(category_list)\n",
    "#             except (ValueError, SyntaxError):\n",
    "#                 category_list = [category_list]\n",
    "#         processed_categories = []\n",
    "#         for category in category_list:\n",
    "#             category = category.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "#             words = category.split()\n",
    "#             stemmed_words = [self.stemmer.stem(word.lower()) for word in words]\n",
    "#             processed_categories.extend(stemmed_words)\n",
    "#         return processed_categories\n",
    "\n",
    "#     def _build_category_vocab(self, categories_list):\n",
    "#         all_categories = [word for categories in categories_list for word in categories]\n",
    "#         category_counter = Counter(all_categories)\n",
    "#         category_vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "#         for index, word in enumerate(category_counter.keys(), start=2):\n",
    "#             category_vocab[word] = index\n",
    "#         return category_vocab\n",
    "\n",
    "#     def _encode_and_pad_categories(self, categories, max_len):\n",
    "#         pad_index = self.category_vocab[\"<pad>\"]\n",
    "#         encoded_categories = [\n",
    "#             self.category_vocab.get(word, self.category_vocab[\"<unk>\"])\n",
    "#             for word in categories\n",
    "#         ]\n",
    "#         if len(encoded_categories) < max_len:\n",
    "#             encoded_categories += [pad_index] * (max_len - len(encoded_categories))\n",
    "#         else:\n",
    "#             encoded_categories = encoded_categories[:max_len]\n",
    "#         return encoded_categories\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.books)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.books.iloc[idx]\n",
    "#         features = {\n",
    "#             \"pageCount\": torch.tensor(row[\"pageCount\"], dtype=torch.float32),\n",
    "#             \"averageRating\": torch.tensor(row[\"averageRating\"], dtype=torch.float32),\n",
    "#             \"ratingsCount\": torch.tensor(row[\"ratingsCount\"], dtype=torch.float32),\n",
    "#             \"author_label\": torch.tensor(row[\"author_label\"], dtype=torch.long),\n",
    "#             \"category_label\": torch.tensor(row[\"category_label\"], dtype=torch.long),\n",
    "#             \"publishedYear\": torch.tensor(row[\"publishedYear\"], dtype=torch.float32),\n",
    "#             \"publisher\": torch.tensor(row[\"publisher\"], dtype=torch.long),\n",
    "#             \"maturityRating\": torch.tensor(row[\"maturityRating\"], dtype=torch.long),\n",
    "#             \"language\": torch.tensor(row[\"language\"], dtype=torch.long),\n",
    "#             \"publisher_missing\": torch.tensor(\n",
    "#                 row[\"publisher_missing\"], dtype=torch.bool\n",
    "#             ),\n",
    "#             \"pageCount_missing\": torch.tensor(\n",
    "#                 row[\"pageCount_missing\"], dtype=torch.bool\n",
    "#             ),\n",
    "#             \"description_missing\": torch.tensor(\n",
    "#                 row[\"description_missing\"], dtype=torch.bool\n",
    "#             ),\n",
    "#             \"categories_missing\": torch.tensor(\n",
    "#                 row[\"categories_missing\"], dtype=torch.bool\n",
    "#             ),\n",
    "#             \"averageRating_missing\": torch.tensor(\n",
    "#                 row[\"averageRating_missing\"], dtype=torch.bool\n",
    "#             ),\n",
    "#             \"ratingsCount_missing\": torch.tensor(\n",
    "#                 row[\"ratingsCount_missing\"], dtype=torch.bool\n",
    "#             ),\n",
    "#             \"full_text_embeddings\": torch.tensor(\n",
    "#                 row[\"full_text_embeddings\"], dtype=torch.float32\n",
    "#             ),\n",
    "#         }\n",
    "#         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = BooksDataset(\"../data/extended_books_google_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/anaconda3/envs/dis_p1/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "original_books = pd.read_csv(\"../data/books_fixed.csv\")\n",
    "book_id = original_books[\"book_id\"]\n",
    "# Step 1: Load the data\n",
    "books = pd.read_csv(\"../data/extended_books_google.csv\")\n",
    "books[\"book_id\"] = book_id\n",
    "\n",
    "# Drop the textSnippet column\n",
    "books = books.drop(columns=[\"textSnippet\"])\n",
    "\n",
    "# Step 2: Build the full_text column\n",
    "books[\"full_text\"] = (\n",
    "    books[\"title\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + books[\"subtitle\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + books[\"description\"].fillna(\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/anaconda3/envs/dis_p1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\n",
    "    \"all-mpnet-base-v2\"\n",
    ")  # You can choose a different model if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efea47beebb4d8190c58abe769c9030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_text_list = books[\"full_text\"].tolist()\n",
    "\n",
    "# Generate embeddings\n",
    "full_text_embeddings = model.encode(full_text_list, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[\"full_text_embeddings\"] = full_text_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>description</th>\n",
       "      <th>pageCount</th>\n",
       "      <th>maturityRating</th>\n",
       "      <th>language</th>\n",
       "      <th>categories</th>\n",
       "      <th>ratingsCount</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>book_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>full_text_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>[-0.012503313831984997, 0.06143873184919357, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu</td>\n",
       "      <td>The Story of the Great Influenza Pandemic of 1...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>Macmillan</td>\n",
       "      <td>1999</td>\n",
       "      <td>\"Scientists have recently discovered shards of...</td>\n",
       "      <td>367.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Medical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Flu The Story of the Great Influenza Pandemic ...</td>\n",
       "      <td>[0.020224615931510925, 0.058842238038778305, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>Putnam Adult</td>\n",
       "      <td>1991</td>\n",
       "      <td>An absorbing narrative of Winnie Louie's life.</td>\n",
       "      <td>424.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>The Kitchen God's Wife  An absorbing narrative...</td>\n",
       "      <td>[0.015043661929666996, 0.0954817458987236, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0440234743</td>\n",
       "      <td>The Testament</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Grisham</td>\n",
       "      <td>Island</td>\n",
       "      <td>1999</td>\n",
       "      <td>Heart of darkness... In a plush Virginia offic...</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Adventure stories</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>The Testament  Heart of darkness... In a plush...</td>\n",
       "      <td>[-0.013089780695736408, 0.05537853762507439, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0452264464</td>\n",
       "      <td>Beloved</td>\n",
       "      <td>A Novel</td>\n",
       "      <td>Toni Morrison</td>\n",
       "      <td>Plume Books</td>\n",
       "      <td>1988</td>\n",
       "      <td>WINNER OF THE NOBEL PRIZE IN LITERATURE.</td>\n",
       "      <td>296.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19</td>\n",
       "      <td>Beloved A Novel WINNER OF THE NOBEL PRIZE IN L...</td>\n",
       "      <td>[0.041055694222450256, 0.001467935391701758, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16594</th>\n",
       "      <td>0786914041</td>\n",
       "      <td>The Spine of the World</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R. A. Salvatore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>The Road to Redemption Even the brutal streets...</td>\n",
       "      <td>381.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248348</td>\n",
       "      <td>The Spine of the World  The Road to Redemption...</td>\n",
       "      <td>[0.04900015518069267, 0.039019856601953506, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16595</th>\n",
       "      <td>0062117378</td>\n",
       "      <td>Pivot Point</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kasie West</td>\n",
       "      <td>HarperTeen</td>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>A girl with the power to see alternate futures...</td>\n",
       "      <td>352.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Juvenile Fiction</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247944</td>\n",
       "      <td>Pivot Point  A girl with the power to see alte...</td>\n",
       "      <td>[0.0005850885645486414, -0.004671064205467701,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16596</th>\n",
       "      <td>1905294964</td>\n",
       "      <td>The Inkheart Trilogy</td>\n",
       "      <td>Inkheart; Inkspell; Inkdeath</td>\n",
       "      <td>Cornelia Caroline Funke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008</td>\n",
       "      <td>Twelve-year-old Meggie learns that her father,...</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Bookbinders</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>248214</td>\n",
       "      <td>The Inkheart Trilogy Inkheart; Inkspell; Inkde...</td>\n",
       "      <td>[0.034835197031497955, -0.05347802862524986, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16597</th>\n",
       "      <td>1937007588</td>\n",
       "      <td>Magic Rises</td>\n",
       "      <td>A Kate Daniels Novel</td>\n",
       "      <td>Ilona Andrews</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>The #1 New York Times Bestseller! Atlanta is a...</td>\n",
       "      <td>370.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247154</td>\n",
       "      <td>Magic Rises A Kate Daniels Novel The #1 New Yo...</td>\n",
       "      <td>[0.007609144784510136, 0.05177111178636551, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16598</th>\n",
       "      <td>0375703845</td>\n",
       "      <td>Glamorama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bret Easton Ellis</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>1998</td>\n",
       "      <td>The New York Times bestselling author of Ameri...</td>\n",
       "      <td>564.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247315</td>\n",
       "      <td>Glamorama  The New York Times bestselling auth...</td>\n",
       "      <td>[0.009258730337023735, 0.07563692331314087, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16599 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ISBN                   title  \\\n",
       "0      0002005018                     NaN   \n",
       "1      0374157065                     Flu   \n",
       "2      0399135782  The Kitchen God's Wife   \n",
       "3      0440234743           The Testament   \n",
       "4      0452264464                 Beloved   \n",
       "...           ...                     ...   \n",
       "16594  0786914041  The Spine of the World   \n",
       "16595  0062117378             Pivot Point   \n",
       "16596  1905294964    The Inkheart Trilogy   \n",
       "16597  1937007588             Magic Rises   \n",
       "16598  0375703845               Glamorama   \n",
       "\n",
       "                                                subtitle  \\\n",
       "0                                                    NaN   \n",
       "1      The Story of the Great Influenza Pandemic of 1...   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                A Novel   \n",
       "...                                                  ...   \n",
       "16594                                                NaN   \n",
       "16595                                                NaN   \n",
       "16596                       Inkheart; Inkspell; Inkdeath   \n",
       "16597                               A Kate Daniels Novel   \n",
       "16598                                                NaN   \n",
       "\n",
       "                       authors     publisher publishedDate  \\\n",
       "0                          NaN           NaN           NaN   \n",
       "1             Gina Bari Kolata     Macmillan          1999   \n",
       "2                      Amy Tan  Putnam Adult          1991   \n",
       "3                 John Grisham        Island          1999   \n",
       "4                Toni Morrison   Plume Books          1988   \n",
       "...                        ...           ...           ...   \n",
       "16594          R. A. Salvatore           NaN          2000   \n",
       "16595               Kasie West    HarperTeen    2013-02-12   \n",
       "16596  Cornelia Caroline Funke           NaN          2008   \n",
       "16597            Ilona Andrews       Penguin    2013-07-30   \n",
       "16598        Bret Easton Ellis       Vintage          1998   \n",
       "\n",
       "                                             description  pageCount  \\\n",
       "0                                                    NaN        NaN   \n",
       "1      \"Scientists have recently discovered shards of...      367.0   \n",
       "2         An absorbing narrative of Winnie Louie's life.      424.0   \n",
       "3      Heart of darkness... In a plush Virginia offic...      550.0   \n",
       "4               WINNER OF THE NOBEL PRIZE IN LITERATURE.      296.0   \n",
       "...                                                  ...        ...   \n",
       "16594  The Road to Redemption Even the brutal streets...      381.0   \n",
       "16595  A girl with the power to see alternate futures...      352.0   \n",
       "16596  Twelve-year-old Meggie learns that her father,...     1146.0   \n",
       "16597  The #1 New York Times Bestseller! Atlanta is a...      370.0   \n",
       "16598  The New York Times bestselling author of Ameri...      564.0   \n",
       "\n",
       "      maturityRating language         categories  ratingsCount  averageRating  \\\n",
       "0                NaN      NaN                NaN           NaN            NaN   \n",
       "1         NOT_MATURE       en            Medical           NaN            NaN   \n",
       "2         NOT_MATURE       en            Fiction           NaN            NaN   \n",
       "3         NOT_MATURE       en  Adventure stories           NaN            NaN   \n",
       "4         NOT_MATURE       en            Fiction           2.0            4.0   \n",
       "...              ...      ...                ...           ...            ...   \n",
       "16594     NOT_MATURE       en            Fiction           NaN            NaN   \n",
       "16595     NOT_MATURE       en   Juvenile Fiction           1.0            3.0   \n",
       "16596     NOT_MATURE       en        Bookbinders           6.0            4.0   \n",
       "16597     NOT_MATURE       en            Fiction           NaN            NaN   \n",
       "16598     NOT_MATURE       en            Fiction           8.0            3.0   \n",
       "\n",
       "       book_id                                          full_text  \\\n",
       "0            1                                                      \n",
       "1            3  Flu The Story of the Great Influenza Pandemic ...   \n",
       "2            5  The Kitchen God's Wife  An absorbing narrative...   \n",
       "3           18  The Testament  Heart of darkness... In a plush...   \n",
       "4           19  Beloved A Novel WINNER OF THE NOBEL PRIZE IN L...   \n",
       "...        ...                                                ...   \n",
       "16594   248348  The Spine of the World  The Road to Redemption...   \n",
       "16595   247944  Pivot Point  A girl with the power to see alte...   \n",
       "16596   248214  The Inkheart Trilogy Inkheart; Inkspell; Inkde...   \n",
       "16597   247154  Magic Rises A Kate Daniels Novel The #1 New Yo...   \n",
       "16598   247315  Glamorama  The New York Times bestselling auth...   \n",
       "\n",
       "                                    full_text_embeddings  \n",
       "0      [-0.012503313831984997, 0.06143873184919357, -...  \n",
       "1      [0.020224615931510925, 0.058842238038778305, 0...  \n",
       "2      [0.015043661929666996, 0.0954817458987236, -0....  \n",
       "3      [-0.013089780695736408, 0.05537853762507439, 0...  \n",
       "4      [0.041055694222450256, 0.001467935391701758, 0...  \n",
       "...                                                  ...  \n",
       "16594  [0.04900015518069267, 0.039019856601953506, -0...  \n",
       "16595  [0.0005850885645486414, -0.004671064205467701,...  \n",
       "16596  [0.034835197031497955, -0.05347802862524986, 0...  \n",
       "16597  [0.007609144784510136, 0.05177111178636551, 0....  \n",
       "16598  [0.009258730337023735, 0.07563692331314087, 0....  \n",
       "\n",
       "[16599 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "books.to_csv(\"../data/extended_books_google_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_file_path = \"../data/extended_books_google_embeddings.csv\"\n",
    "books_data = pd.read_csv(books_file_path)\n",
    "\n",
    "# Load the training data\n",
    "train_file_path = \"../data/train.csv\"\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "\n",
    "merged_data = train_data.merge(books_data, on=\"book_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>description</th>\n",
       "      <th>pageCount</th>\n",
       "      <th>maturityRating</th>\n",
       "      <th>language</th>\n",
       "      <th>categories</th>\n",
       "      <th>ratingsCount</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>full_text</th>\n",
       "      <th>full_text_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7260</td>\n",
       "      <td>20145</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0316171638</td>\n",
       "      <td>Tangled Vines</td>\n",
       "      <td>A Novel</td>\n",
       "      <td>Janet Dailey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1992</td>\n",
       "      <td>Story of television news personality Kelly Dou...</td>\n",
       "      <td>363.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>California</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tangled Vines A Novel Story of television news...</td>\n",
       "      <td>[-0.0026500036474317312, 0.01970168761909008, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243238</td>\n",
       "      <td>85182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0316166685</td>\n",
       "      <td>The Lovely Bones</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alice Sebold</td>\n",
       "      <td>Grand Central Pub</td>\n",
       "      <td>2006</td>\n",
       "      <td>The spirit of fourteen-year-old Susie Salmon d...</td>\n",
       "      <td>372.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The Lovely Bones  The spirit of fourteen-year-...</td>\n",
       "      <td>[0.06080690026283264, 0.04353366419672966, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9135</td>\n",
       "      <td>45973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0446692298</td>\n",
       "      <td>Fat Girls and Lawn Chairs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cheryl Peck</td>\n",
       "      <td>Grand Central Publishing</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>Cheryl Peck has many stories to tell-of her na...</td>\n",
       "      <td>256.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Biography &amp; Autobiography</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fat Girls and Lawn Chairs  Cheryl Peck has man...</td>\n",
       "      <td>[0.012768053449690342, 0.058900486677885056, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18671</td>\n",
       "      <td>63554</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0440944597</td>\n",
       "      <td>The Chocolate War</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Robert Cormier</td>\n",
       "      <td>Laurel Leaf</td>\n",
       "      <td>1974</td>\n",
       "      <td>A high-school freshman who refuses to particip...</td>\n",
       "      <td>274.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Juvenile Fiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Chocolate War  A high-school freshman who ...</td>\n",
       "      <td>[0.037530988454818726, 0.10081930458545685, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>243293</td>\n",
       "      <td>81002</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0439244196</td>\n",
       "      <td>Holes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis Sachar</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>1998</td>\n",
       "      <td>As further evidence of his family's bad fortun...</td>\n",
       "      <td>244.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Juvenile Fiction</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Holes  As further evidence of his family's bad...</td>\n",
       "      <td>[0.012188694439828396, -0.02690301276743412, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100518</th>\n",
       "      <td>15374</td>\n",
       "      <td>69658</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1551660202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>[-0.012503313831984997, 0.06143873184919357, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100519</th>\n",
       "      <td>11063</td>\n",
       "      <td>69658</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0515130966</td>\n",
       "      <td>Riptide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Catherine Coulter</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>2001-07-01</td>\n",
       "      <td>Agents Dillon Savich and Lacey Sherlock must p...</td>\n",
       "      <td>372.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Riptide  Agents Dillon Savich and Lacey Sherlo...</td>\n",
       "      <td>[-0.008309813216328621, 0.03894886374473572, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100520</th>\n",
       "      <td>18444</td>\n",
       "      <td>29981</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0441005748</td>\n",
       "      <td>Blue Moon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Laurell K. Hamilton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1998</td>\n",
       "      <td>Guilt or innocence is not the issue when Anita...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Blake, Anita (Fictitious character)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Blue Moon  Guilt or innocence is not the issue...</td>\n",
       "      <td>[0.055790651589632034, 0.026199055835604668, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100521</th>\n",
       "      <td>5917</td>\n",
       "      <td>38009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0671039830</td>\n",
       "      <td>Eye Of The Storm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V.C. Andrews</td>\n",
       "      <td>Pocket Books</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>In the wake of a terrible loss, Rain is left a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eye Of The Storm  In the wake of a terrible lo...</td>\n",
       "      <td>[0.008353191427886486, 0.04196145758032799, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100522</th>\n",
       "      <td>243328</td>\n",
       "      <td>94374</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0670026603</td>\n",
       "      <td>Me Before You</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jojo Moyes</td>\n",
       "      <td>Pamela Dorman Books</td>\n",
       "      <td>2012</td>\n",
       "      <td>Louisa Clark is an ordinary girl living an exc...</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NOT_MATURE</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Me Before You  Louisa Clark is an ordinary gir...</td>\n",
       "      <td>[-0.014953081496059895, 0.04887177795171738, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100523 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        book_id  user_id  rating        ISBN                      title  \\\n",
       "0          7260    20145     3.5  0316171638              Tangled Vines   \n",
       "1        243238    85182     4.0  0316166685           The Lovely Bones   \n",
       "2          9135    45973     1.0  0446692298  Fat Girls and Lawn Chairs   \n",
       "3         18671    63554     3.0  0440944597          The Chocolate War   \n",
       "4        243293    81002     5.0  0439244196                      Holes   \n",
       "...         ...      ...     ...         ...                        ...   \n",
       "100518    15374    69658     2.0  1551660202                        NaN   \n",
       "100519    11063    69658     2.5  0515130966                    Riptide   \n",
       "100520    18444    29981     1.0  0441005748                  Blue Moon   \n",
       "100521     5917    38009     1.0  0671039830           Eye Of The Storm   \n",
       "100522   243328    94374     4.0  0670026603              Me Before You   \n",
       "\n",
       "       subtitle              authors                 publisher publishedDate  \\\n",
       "0       A Novel         Janet Dailey                       NaN          1992   \n",
       "1           NaN         Alice Sebold         Grand Central Pub          2006   \n",
       "2           NaN          Cheryl Peck  Grand Central Publishing    2004-01-01   \n",
       "3           NaN       Robert Cormier               Laurel Leaf          1974   \n",
       "4           NaN         Louis Sachar                Scholastic          1998   \n",
       "...         ...                  ...                       ...           ...   \n",
       "100518      NaN                  NaN                       NaN           NaN   \n",
       "100519      NaN    Catherine Coulter                   Penguin    2001-07-01   \n",
       "100520      NaN  Laurell K. Hamilton                       NaN          1998   \n",
       "100521      NaN         V.C. Andrews              Pocket Books    2000-11-01   \n",
       "100522      NaN           Jojo Moyes       Pamela Dorman Books          2012   \n",
       "\n",
       "                                              description  pageCount  \\\n",
       "0       Story of television news personality Kelly Dou...      363.0   \n",
       "1       The spirit of fourteen-year-old Susie Salmon d...      372.0   \n",
       "2       Cheryl Peck has many stories to tell-of her na...      256.0   \n",
       "3       A high-school freshman who refuses to particip...      274.0   \n",
       "4       As further evidence of his family's bad fortun...      244.0   \n",
       "...                                                   ...        ...   \n",
       "100518                                                NaN        NaN   \n",
       "100519  Agents Dillon Savich and Lacey Sherlock must p...      372.0   \n",
       "100520  Guilt or innocence is not the issue when Anita...      418.0   \n",
       "100521  In the wake of a terrible loss, Rain is left a...        0.0   \n",
       "100522  Louisa Clark is an ordinary girl living an exc...      385.0   \n",
       "\n",
       "       maturityRating language                           categories  \\\n",
       "0          NOT_MATURE       en                           California   \n",
       "1          NOT_MATURE       en                              Fiction   \n",
       "2          NOT_MATURE       en            Biography & Autobiography   \n",
       "3          NOT_MATURE       en                     Juvenile Fiction   \n",
       "4          NOT_MATURE       en                     Juvenile Fiction   \n",
       "...               ...      ...                                  ...   \n",
       "100518            NaN      NaN                                  NaN   \n",
       "100519     NOT_MATURE       en                              Fiction   \n",
       "100520     NOT_MATURE       en  Blake, Anita (Fictitious character)   \n",
       "100521     NOT_MATURE       en                              Fiction   \n",
       "100522     NOT_MATURE       en                              Fiction   \n",
       "\n",
       "        ratingsCount  averageRating  \\\n",
       "0                NaN            NaN   \n",
       "1                1.0            3.0   \n",
       "2                NaN            NaN   \n",
       "3                NaN            NaN   \n",
       "4                1.0            4.0   \n",
       "...              ...            ...   \n",
       "100518           NaN            NaN   \n",
       "100519           3.0            4.0   \n",
       "100520           1.0            5.0   \n",
       "100521           NaN            NaN   \n",
       "100522           NaN            NaN   \n",
       "\n",
       "                                                full_text  \\\n",
       "0       Tangled Vines A Novel Story of television news...   \n",
       "1       The Lovely Bones  The spirit of fourteen-year-...   \n",
       "2       Fat Girls and Lawn Chairs  Cheryl Peck has man...   \n",
       "3       The Chocolate War  A high-school freshman who ...   \n",
       "4       Holes  As further evidence of his family's bad...   \n",
       "...                                                   ...   \n",
       "100518                                                      \n",
       "100519  Riptide  Agents Dillon Savich and Lacey Sherlo...   \n",
       "100520  Blue Moon  Guilt or innocence is not the issue...   \n",
       "100521  Eye Of The Storm  In the wake of a terrible lo...   \n",
       "100522  Me Before You  Louisa Clark is an ordinary gir...   \n",
       "\n",
       "                                     full_text_embeddings  \n",
       "0       [-0.0026500036474317312, 0.01970168761909008, ...  \n",
       "1       [0.06080690026283264, 0.04353366419672966, 0.0...  \n",
       "2       [0.012768053449690342, 0.058900486677885056, 0...  \n",
       "3       [0.037530988454818726, 0.10081930458545685, 0....  \n",
       "4       [0.012188694439828396, -0.02690301276743412, -...  \n",
       "...                                                   ...  \n",
       "100518  [-0.012503313831984997, 0.06143873184919357, -...  \n",
       "100519  [-0.008309813216328621, 0.03894886374473572, 0...  \n",
       "100520  [0.055790651589632034, 0.026199055835604668, -...  \n",
       "100521  [0.008353191427886486, 0.04196145758032799, 0....  \n",
       "100522  [-0.014953081496059895, 0.04887177795171738, 0...  \n",
       "\n",
       "[100523 rows x 18 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
